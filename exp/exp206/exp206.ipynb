{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "182844c7-8eff-41c0-9847-4ccbe7975b19",
   "metadata": {},
   "source": [
    "# Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c076122-4690-4781-a486-e699bed02d69",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gc\n",
    "import random\n",
    "import joblib\n",
    "import pickle\n",
    "from typing import List, Dict, Union\n",
    "\n",
    "import numpy as np\n",
    "from numba import njit\n",
    "import polars as pl\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import GroupKFold\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from scripts.metrics import map_at_k, ap_at_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "298c78ad-edda-49c1-a972-842d08660152",
   "metadata": {},
   "outputs": [],
   "source": [
    "EXP_NAME = \"exp206\"\n",
    "INPUT_DIR = \"../../input/raw/\"\n",
    "CANDIDATES_DIR = \"candidates/\"\n",
    "FEATURE_DIR = \"features/\"\n",
    "LAST_NS = [1, 2, 3]\n",
    "K_FOLDS = 5\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d40c8df-26e4-4a10-a2b5-1dd26cb0ba00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "seed_everything(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d1e8d13b-cfb8-497b-ab8d-394018c86677",
   "metadata": {},
   "outputs": [],
   "source": [
    "LGBM_PARAMS = {\n",
    "    \"objective\": \"lambdarank\",\n",
    "    \"metric\": \"map\",\n",
    "    \"boosting\": \"dart\",\n",
    "    \"seed\": SEED,\n",
    "    \"num_leaves\": 13,\n",
    "    \"max_depth\": 7,\n",
    "    \"learning_rate\": 0.1,\n",
    "    \"feature_fraction\": 0.9,\n",
    "    \"bagging_fraction\": 0.9,\n",
    "    \"bagging_freq\": 5,\n",
    "    \"min_data_in_leaf\": 30,\n",
    "    \"eval_at\": [20],\n",
    "    # \"lambdarank_truncation_level\" : 40,\n",
    "    \"deterministic\":True,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "828f5d6b-41d1-4702-9e60-5ca589b3528a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_log = pl.read_csv(os.path.join(INPUT_DIR, \"train_log.csv\"))\n",
    "train_label = pl.read_csv(os.path.join(INPUT_DIR, \"train_label.csv\"))\n",
    "test_log = pl.read_csv(os.path.join(INPUT_DIR, \"test_log.csv\"))\n",
    "test_session = pl.read_csv(os.path.join(INPUT_DIR, \"test_session.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "99b9bacc-af08-4a3c-aa0a-34e38016ec06",
   "metadata": {},
   "outputs": [],
   "source": [
    "co_visit_matrix_train = pl.read_parquet(os.path.join(CANDIDATES_DIR, \"co_visit_matrix_for_train_or_eval.parquet\"))\n",
    "co_visit_matrix_test = pl.read_parquet(os.path.join(CANDIDATES_DIR, \"co_visit_matrix_for_test.parquet\"))\n",
    "\n",
    "co_visit_matrix_trend_train = pl.read_parquet(os.path.join(CANDIDATES_DIR, \"co_visit_matrix_trend_for_train_or_eval.parquet\"))\n",
    "co_visit_matrix_trend_test = pl.read_parquet(os.path.join(CANDIDATES_DIR, \"co_visit_matrix_trend_for_test.parquet\"))\n",
    "\n",
    "already_clicked_train = pl.read_parquet(os.path.join(CANDIDATES_DIR, \"already_clicked_for_train_or_eval.parquet\"))\n",
    "already_clicked_test = pl.read_parquet(os.path.join(CANDIDATES_DIR, \"already_clicked_for_test.parquet\"))\n",
    "\n",
    "popular_yados_train = pl.read_parquet(os.path.join(CANDIDATES_DIR, \"popular_yados_per_sml_cd_for_train_or_eval.parquet\"))\n",
    "popular_yados_test = pl.read_parquet(os.path.join(CANDIDATES_DIR, \"popular_yados_per_sml_cd_for_test.parquet\"))\n",
    "\n",
    "popular_yados_trend_train = pl.read_parquet(os.path.join(CANDIDATES_DIR, \"popular_yados_trend_per_sml_cd_for_train_or_eval.parquet\"))\n",
    "popular_yados_trend_test = pl.read_parquet(os.path.join(CANDIDATES_DIR, \"popular_yados_trend_per_sml_cd_for_test.parquet\"))\n",
    "\n",
    "similar_yados = pl.read_parquet(os.path.join(CANDIDATES_DIR, \"similar_yados.parquet\"))\n",
    "\n",
    "imf_train = pl.read_parquet(os.path.join(CANDIDATES_DIR, \"imf_for_train_or_eval.parquet\"))\n",
    "imf_test = pl.read_parquet(os.path.join(CANDIDATES_DIR, \"imf_for_test.parquet\"))\n",
    "\n",
    "trend_imf_train = pl.read_parquet(os.path.join(CANDIDATES_DIR, \"trend_imf_for_train_or_eval.parquet\"))\n",
    "trend_imf_test = pl.read_parquet(os.path.join(CANDIDATES_DIR, \"trend_imf_for_test.parquet\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f4456ff3-b3f5-4594-8e62-ec3d1e1871f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CandidateMatrix:\n",
    "    def __init__(self, matrix: pl.DataFrame, feat_name: List[str], join_key: str):\n",
    "        self.matrix = matrix\n",
    "        self.feat_name = feat_name\n",
    "        self.join_key = join_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "73476554-585e-4630-83dd-a5c3c3894cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate_matrices_train =[\n",
    "    CandidateMatrix(co_visit_matrix_train, [\"co_visit_weight_rank\"], \"item\"),\n",
    "    CandidateMatrix(co_visit_matrix_trend_train, [\"trend_co_visit_weight_rank\"], \"item\"),\n",
    "    CandidateMatrix(already_clicked_train, [\"seq_no_inverse\"], \"session\"),\n",
    "    CandidateMatrix(popular_yados_train, [\"popularity_rank\"], \"item\"),\n",
    "    CandidateMatrix(popular_yados_trend_train, [\"trend_popularity_rank\"], \"item\"),\n",
    "    CandidateMatrix(similar_yados, [\"nn_distance\"], \"item\"),\n",
    "    CandidateMatrix(imf_train, [\"imf_rank\"], \"session\"),\n",
    "    CandidateMatrix(trend_imf_train, [\"trend_imf_rank\"], \"session\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "19bd0784-0597-44d9-8274-d31bb45c251c",
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate_matrices_test =[\n",
    "    CandidateMatrix(co_visit_matrix_test, [\"co_visit_weight_rank\"], \"item\"),\n",
    "    CandidateMatrix(co_visit_matrix_trend_test, [\"trend_co_visit_weight_rank\"], \"item\"),\n",
    "    CandidateMatrix(already_clicked_test, [\"seq_no_inverse\"], \"session\"),\n",
    "    CandidateMatrix(popular_yados_test, [\"popularity_rank\"], \"item\"),\n",
    "    CandidateMatrix(popular_yados_trend_test, [\"trend_popularity_rank\"], \"item\"),\n",
    "    CandidateMatrix(similar_yados, [\"nn_distance\"], \"item\"),\n",
    "    CandidateMatrix(imf_test, [\"imf_rank\"], \"session\"),\n",
    "    CandidateMatrix(trend_imf_test, [\"trend_imf_rank\"], \"session\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "40f2fd77-8538-4d36-9259-e5e9c0680fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "session_feat_train = pl.read_parquet(os.path.join(FEATURE_DIR, \"session_feat_for_train_or_eval.parquet\"))\n",
    "session_feat_test = pl.read_parquet(os.path.join(FEATURE_DIR, \"session_feat_for_test.parquet\"))\n",
    "product_feat_train = pl.read_parquet(os.path.join(FEATURE_DIR, \"product_feat_train.parquet\"))\n",
    "product_feat_test = pl.read_parquet(os.path.join(FEATURE_DIR, \"product_feat_test.parquet\"))\n",
    "session_product_feat_train = pl.read_parquet(os.path.join(FEATURE_DIR, \"session_product_feat_for_train_or_eval.parquet\"))\n",
    "session_product_feat_test = pl.read_parquet(os.path.join(FEATURE_DIR, \"session_product_feat_for_test.parquet\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4b71b7fd-33bc-4db6-9f88-0e65cc644a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imf\n",
    "imf_model_train = np.load(os.path.join(FEATURE_DIR, \"imf_model_for_train_or_eval.npz\"))\n",
    "with open(os.path.join(FEATURE_DIR, \"imf_user_id2index_for_train_or_eval.pickle\"), \"rb\") as f:\n",
    "    user_id2index_train = pickle.load(f)\n",
    "with open(os.path.join(FEATURE_DIR, \"imf_item_id2index_for_train_or_eval.pickle\"), \"rb\") as f:\n",
    "    item_id2index_train = pickle.load(f)\n",
    "\n",
    "imf_model_test = np.load(os.path.join(FEATURE_DIR, \"imf_model_for_test.npz\"))\n",
    "with open(os.path.join(FEATURE_DIR, \"imf_user_id2index_for_test.pickle\"), \"rb\") as f:\n",
    "    user_id2index_test = pickle.load(f)\n",
    "with open(os.path.join(FEATURE_DIR, \"imf_item_id2index_for_test.pickle\"), \"rb\") as f:\n",
    "    item_id2index_test = pickle.load(f)\n",
    "\n",
    "trend_imf_model_train = np.load(os.path.join(FEATURE_DIR, \"trend_imf_model_for_train_or_eval.npz\"))\n",
    "with open(os.path.join(FEATURE_DIR, \"trend_imf_user_id2index_for_train_or_eval.pickle\"), \"rb\") as f:\n",
    "    trend_user_id2index_train = pickle.load(f)\n",
    "with open(os.path.join(FEATURE_DIR, \"trend_imf_item_id2index_for_train_or_eval.pickle\"), \"rb\") as f:\n",
    "    trend_item_id2index_train = pickle.load(f)\n",
    "\n",
    "trend_imf_model_test = np.load(os.path.join(FEATURE_DIR, \"trend_imf_model_for_test.npz\"))\n",
    "with open(os.path.join(FEATURE_DIR, \"trend_imf_user_id2index_for_test.pickle\"), \"rb\") as f:\n",
    "    trend_user_id2index_test = pickle.load(f)\n",
    "with open(os.path.join(FEATURE_DIR, \"trend_imf_item_id2index_for_test.pickle\"), \"rb\") as f:\n",
    "    trend_item_id2index_test = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "210c619d-5e39-468e-941b-950dfb8f2b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_candidates(df: pl.DataFrame, candidate_matrices:List[CandidateMatrix]) -> pl.DataFrame:\n",
    "\n",
    "    def add_last_n_item(df: pl.DataFrame, last_n: int) -> pl.DataFrame:\n",
    "        last_item_list = []\n",
    "        prev_items_list = df[\"prev_items\"].to_list()\n",
    "        for prev_items in prev_items_list:\n",
    "            try:\n",
    "                last_item_list.append(prev_items[-last_n])\n",
    "            except IndexError:\n",
    "                last_item_list.append(None)\n",
    "        df = df.with_columns(pl.Series(name=f\"last_yad_no_{last_n}\", values=last_item_list))\n",
    "        return df\n",
    "\n",
    "    # add last_item columns\n",
    "    for last_n in LAST_NS:\n",
    "        df = add_last_n_item(df, last_n)\n",
    "\n",
    "    # generate candidates\n",
    "    candidates = []\n",
    "\n",
    "    # candidates tied to items\n",
    "    for last_n in LAST_NS:\n",
    "        for candidate_matrix in candidate_matrices:\n",
    "            if candidate_matrix.join_key == \"item\":        \n",
    "                # join candidates to last_n item\n",
    "                candidate = df.join(candidate_matrix.matrix, left_on=f\"last_yad_no_{last_n}\", right_on=\"yad_no\", how=\"left\")\n",
    "                candidate = candidate.filter(pl.col(\"last_yad_no_1\") != pl.col(\"candidate_yad_no\")) # remove last click yado\n",
    "                \n",
    "                # keep candidates for feature addition later\n",
    "                features_original = candidate_matrix.feat_name\n",
    "                features = [f\"{x}_last{last_n}\" for x in features_original]\n",
    "                tmp = candidate[[\"session_id\", \"candidate_yad_no\"] + features_original]\n",
    "                for feature, feature_original in zip(features, features_original):\n",
    "                    tmp = tmp.rename({feature_original:feature})\n",
    "                candidates.append(tmp)\n",
    "\n",
    "    # candidates tied to session\n",
    "    for candidate_matrix in candidate_matrices:\n",
    "        if candidate_matrix.join_key == \"session\":\n",
    "            candidate = df.join(candidate_matrix.matrix, on=\"session_id\", how=\"left\")\n",
    "            candidate = candidate.filter(pl.col(\"last_yad_no_1\") != pl.col(\"candidate_yad_no\")) # remove last click yado\n",
    "            candidates.append(candidate[[\"session_id\", \"candidate_yad_no\"] + candidate_matrix.feat_name])\n",
    "\n",
    "    # concatenate candidates\n",
    "    cand_all = pl.concat([df[[\"session_id\", \"candidate_yad_no\"]] for df in candidates])\n",
    "\n",
    "    # remove duplicate candidates\n",
    "    cand_all = cand_all.unique(subset=[\"session_id\", \"candidate_yad_no\"])\n",
    "\n",
    "    # join candidates\n",
    "    df = df.join(cand_all, on=[\"session_id\"], how=\"left\")\n",
    "\n",
    "    # add features derived from the candidate\n",
    "    for candidate in candidates:\n",
    "        df = df.join(candidate, on=[\"session_id\", \"candidate_yad_no\"], how=\"left\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "12c4c4e9-90ad-409c-8d36-60d4ed7cee3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_label(df: pl.DataFrame) -> pl.DataFrame:\n",
    "    df = df.with_columns((pl.col(\"candidate_yad_no\") == pl.col(\"label\")).cast(pl.Int8).alias(\"label\"))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ab0e2160-7ee7-4794-ac98-2bf216d8bf8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_null(df: pl.DataFrame, candidate_matrices:List[CandidateMatrix]) -> pl.DataFrame:\n",
    "    feat_names = []\n",
    "    for candidate_matrix in candidate_matrices:\n",
    "        if candidate_matrix.join_key == \"item\":\n",
    "            for last_n in LAST_NS:\n",
    "                for feat_name in candidate_matrix.feat_name:\n",
    "                    feat_names.append(f\"{feat_name}_last{last_n}\")\n",
    "        elif candidate_matrix.join_key == \"session\":\n",
    "            feat_names.extend(candidate_matrix.feat_name)\n",
    "    \n",
    "    df = df.filter(\n",
    "        ~pl.all_horizontal(pl.col(feat_names).is_null())\n",
    "    )\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2c320214-f9f5-4dc6-91d4-f277d1384dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_session_not_include_positive(df: pl.DataFrame) -> pl.DataFrame:\n",
    "    positive_sessions = df.filter(pl.col(\"label\")==1)[\"session_id\"].to_list()\n",
    "    df = df.filter(df[\"session_id\"].is_in(positive_sessions))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6aa0cbb7-7bd9-412a-824f-42cf8fdb5f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def negative_sample(df: pl.DataFrame) -> pl.DataFrame:\n",
    "    negatives = df.filter(df[\"label\"] == 0)\n",
    "    negatives = negatives.sample(fraction=0.025, seed=SEED)\n",
    "    df = pl.concat([df.filter(df[\"label\"] > 0), negatives])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "23bfc534-3fc9-4068-8ee6-8383673099e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_features(\n",
    "    df: pl.DataFrame, \n",
    "    session_feat_df:pl.DataFrame, product_feat_df:pl.DataFrame, session_product_feat_df:pl.DataFrame,\n",
    "    imf_model, user_id2index, item_id2index,\n",
    "    trend_imf_model, trend_user_id2index, trend_item_id2index) -> pl.DataFrame:\n",
    "\n",
    "    ### session features\n",
    "    df = df.join(session_feat_df, on=\"session_id\", how=\"left\")\n",
    "\n",
    "    ### product features\n",
    "    df = df.join(product_feat_df, left_on=\"candidate_yad_no\", right_on=\"yad_no\", how=\"left\")\n",
    "\n",
    "    ### session * product features\n",
    "    df = df.join(session_product_feat_df, left_on=[\"session_id\", \"candidate_yad_no\"], right_on=[\"session_id\", \"yad_no\"], how=\"left\")\n",
    "\n",
    "    # item2item similality\n",
    "    for last_n in LAST_NS:\n",
    "        df = df.with_columns([\n",
    "            ((pl.col(\"P_wireless_lan_flg\") == pl.col(f\"S_wireless_lan_flg_last{last_n}\"))&(pl.col(f\"S_wireless_lan_flg_last{last_n}\").is_not_null())).cast(pl.UInt8).alias(f\"SP_same_lan_flg_last{last_n}\"),\n",
    "            ((pl.col(\"P_onsen_flg\") == pl.col(f\"S_onsen_flg_last{last_n}\"))&(pl.col(f\"S_onsen_flg_last{last_n}\").is_not_null())).cast(pl.UInt8).alias(f\"SP_same_onsen_flg_last{last_n}\"),\n",
    "            ((pl.col(\"P_kd_stn_5min\") == pl.col(f\"S_kd_stn_5min_last{last_n}\"))&(pl.col(f\"S_kd_stn_5min_last{last_n}\").is_not_null())).cast(pl.UInt8).alias(f\"SP_same_kd_stn_5min_last{last_n}\"),\n",
    "            ((pl.col(\"P_kd_bch_5min\") == pl.col(f\"S_kd_bch_5min_last{last_n}\"))&(pl.col(f\"S_kd_bch_5min_last{last_n}\").is_not_null())).cast(pl.UInt8).alias(f\"SP_same_kd_bch_5min_last{last_n}\"),\n",
    "            ((pl.col(\"P_kd_slp_5min\") == pl.col(f\"S_kd_slp_5min_last{last_n}\"))&(pl.col(f\"S_kd_slp_5min_last{last_n}\").is_not_null())).cast(pl.UInt8).alias(f\"SP_same_kd_slp_5min_last{last_n}\"),\n",
    "            ((pl.col(\"P_kd_conv_walk_5min\") == pl.col(f\"S_kd_conv_walk_5min_last{last_n}\"))&(pl.col(f\"S_kd_conv_walk_5min_last{last_n}\").is_not_null())).cast(pl.UInt8).alias(f\"SP_same_kd_conv_walk_5min_last{last_n}\"),\n",
    "            ((pl.col(\"P_wid_cd\") == pl.col(f\"S_wid_cd_last{last_n}\"))&(pl.col(f\"S_wid_cd_last{last_n}\").is_not_null())).cast(pl.UInt8).alias(f\"SP_same_wid_cd_last{last_n}\"),\n",
    "            ((pl.col(\"P_ken_cd\") == pl.col(f\"S_ken_cd_last{last_n}\"))&(pl.col(f\"S_ken_cd_last{last_n}\").is_not_null())).cast(pl.UInt8).alias(f\"SP_same_ken_cd_last{last_n}\"),\n",
    "            ((pl.col(\"P_lrg_cd\") == pl.col(f\"S_lrg_cd_last{last_n}\"))&(pl.col(f\"S_lrg_cd_last{last_n}\").is_not_null())).cast(pl.UInt8).alias(f\"SP_same_lrg_cd_last{last_n}\"),\n",
    "            ((pl.col(\"P_sml_cd\") == pl.col(f\"S_sml_cd_last{last_n}\"))&(pl.col(f\"S_sml_cd_last{last_n}\").is_not_null())).cast(pl.UInt8).alias(f\"SP_same_sml_cd_last{last_n}\"),\n",
    "        ])\n",
    "\n",
    "    for last_n in LAST_NS:\n",
    "        df = df.with_columns(\n",
    "            pl.sum_horizontal(\n",
    "                f\"SP_same_lan_flg_last{last_n}\", \n",
    "                f\"SP_same_onsen_flg_last{last_n}\", \n",
    "                f\"SP_same_kd_stn_5min_last{last_n}\", \n",
    "                f\"SP_same_kd_bch_5min_last{last_n}\", \n",
    "                f\"SP_same_kd_slp_5min_last{last_n}\", \n",
    "                f\"SP_same_kd_conv_walk_5min_last{last_n}\", \n",
    "                f\"SP_same_wid_cd_last{last_n}\", \n",
    "                f\"SP_same_ken_cd_last{last_n}\", \n",
    "                f\"SP_same_lan_flg_last{last_n}\", \n",
    "                f\"SP_same_lrg_cd_last{last_n}\", \n",
    "            )\n",
    "            .alias(f\"SP_item2item_similarity_last{last_n}\")\n",
    "        )\n",
    "    df = df.with_columns(\n",
    "        df[[f\"SP_item2item_similarity_last{last_n}\" for last_n in LAST_NS]].mean(axis=1).alias(f\"SP_item2item_similarity\")\n",
    "    )\n",
    "    df = df.with_columns(\n",
    "        (pl.col(\"P_total_room_cnt\") - pl.col(\"S_mean_room_cnt\")).alias(\"SP_diff_room_cnt\")\n",
    "    )\n",
    "\n",
    "    # session2item similarity\n",
    "    @njit()\n",
    "    def calc_cos_sim(v1, v2):\n",
    "        return np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))\n",
    "\n",
    "    sessions = df[\"session_id\"].to_list()\n",
    "    candidates = df[\"candidate_yad_no\"].to_list()\n",
    "    user_index2vector = dict(enumerate(imf_model[\"user_factors\"]))\n",
    "    item_index2vector = dict(enumerate(imf_model[\"item_factors\"]))\n",
    "    imf_similarities = []\n",
    "    for session, candidate in zip(sessions, candidates):\n",
    "        try:\n",
    "            user_index, item_index = user_id2index[session], item_id2index[candidate]\n",
    "            v1, v2 = user_index2vector[user_index], item_index2vector[item_index]\n",
    "            sim = calc_cos_sim(v1, v2)\n",
    "        except (KeyError, TypeError): # KeyError if the item is not in the imf training data. TypeError if there are no candidates in a session.\n",
    "            sim = -1\n",
    "        imf_similarities.append(np.float32(sim))\n",
    "    df = df.with_columns(pl.Series(name=\"imf_similarity\", values=imf_similarities).cast(pl.Float32))\n",
    "\n",
    "    sessions = df[\"session_id\"].to_list()\n",
    "    candidates = df[\"candidate_yad_no\"].to_list()\n",
    "    user_index2vector = dict(enumerate(trend_imf_model[\"user_factors\"]))\n",
    "    item_index2vector = dict(enumerate(trend_imf_model[\"item_factors\"]))\n",
    "    imf_similarities = []\n",
    "    for session, candidate in zip(sessions, candidates):\n",
    "        try:\n",
    "            user_index, item_index = trend_user_id2index[session], trend_item_id2index[candidate]\n",
    "            v1, v2 = user_index2vector[user_index], item_index2vector[item_index]\n",
    "            sim = calc_cos_sim(v1, v2)\n",
    "        except (KeyError, TypeError): # KeyError if the item is not in the imf training data. TypeError if there are no candidates in a session.\n",
    "            sim = -1\n",
    "        imf_similarities.append(np.float32(sim))\n",
    "    df = df.with_columns(pl.Series(name=\"trend_imf_similarity\", values=imf_similarities).cast(pl.Float32))\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7a62aaa9-274b-4944-bb60-1afa6273dee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_null_and_cast(df: pl.DataFrame) -> pl.DataFrame:\n",
    "    df = df.with_columns([\n",
    "        pl.col(\"co_visit_weight_rank_last1\").fill_null(999).cast(pl.Float32),\n",
    "        pl.col(\"co_visit_weight_rank_last2\").fill_null(999).cast(pl.Float32),\n",
    "        pl.col(\"co_visit_weight_rank_last3\").fill_null(999).cast(pl.Float32),\n",
    "        pl.col(\"trend_co_visit_weight_rank_last1\").fill_null(999).cast(pl.Float32),\n",
    "        pl.col(\"trend_co_visit_weight_rank_last2\").fill_null(999).cast(pl.Float32),\n",
    "        pl.col(\"trend_co_visit_weight_rank_last3\").fill_null(999).cast(pl.Float32),\n",
    "        pl.col(\"popularity_rank_last1\").fill_null(999).cast(pl.Float32),\n",
    "        pl.col(\"popularity_rank_last2\").fill_null(999).cast(pl.Float32),\n",
    "        pl.col(\"popularity_rank_last3\").fill_null(999).cast(pl.Float32),\n",
    "        pl.col(\"trend_popularity_rank_last1\").fill_null(999).cast(pl.Float32),\n",
    "        pl.col(\"trend_popularity_rank_last2\").fill_null(999).cast(pl.Float32),\n",
    "        pl.col(\"trend_popularity_rank_last3\").fill_null(999).cast(pl.Float32),\n",
    "        pl.col(\"nn_distance_last1\").fill_null(999).cast(pl.Float32),\n",
    "        pl.col(\"nn_distance_last2\").fill_null(999).cast(pl.Float32),\n",
    "        pl.col(\"nn_distance_last3\").fill_null(999).cast(pl.Float32),\n",
    "        pl.col(\"seq_no_inverse\").fill_null(999).cast(pl.Int16),\n",
    "        pl.col(\"imf_rank\").fill_null(999).cast(pl.Int16),\n",
    "        pl.col(\"trend_imf_rank\").fill_null(999).cast(pl.Int16),\n",
    "        pl.col(\"SP_interact_count\").fill_null(0).cast(pl.Int16),\n",
    "    ])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7a43c9b5-3ee6-4f14-a551-e4c5d54338c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(df:pl.DataFrame, chunk_size:int=30_000_000) -> np.array:\n",
    "    preds = np.zeros((df.shape[0],))\n",
    "    for fold in range(K_FOLDS):\n",
    "        # load model\n",
    "        model = pickle.load(open(f'../../model/lgb_{EXP_NAME}_{fold+1}.pkl', \"rb\"))\n",
    "        # chunk data and predict to prevent OOM\n",
    "        preds_by_one_model = []\n",
    "        for frame in df.iter_slices(n_rows=chunk_size):\n",
    "            preds_chunk = model.predict(frame[FEATURES].to_pandas(), num_iteration=model.best_iteration)\n",
    "            preds_by_one_model.append(preds_chunk)\n",
    "        preds += np.concatenate(preds_by_one_model)\n",
    "    preds /= K_FOLDS\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c6f028c-7723-4747-b821-a3215b0c8f95",
   "metadata": {},
   "source": [
    "# Process train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cf9a386d-6cff-467f-9a1d-22964836e975",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_train_log = train_log.clone()\n",
    "original_train_label = train_label.clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "40d8f3c2-8c1f-4691-be5d-6a3839efa414",
   "metadata": {},
   "outputs": [],
   "source": [
    "prev_items_list = (\n",
    "    train_log\n",
    "    .sort([\"session_id\", \"seq_no\"])\n",
    "    .group_by(\"session_id\", maintain_order=True)\n",
    "    .agg(pl.col(\"yad_no\"))\n",
    ")[\"yad_no\"].to_list()\n",
    "\n",
    "train = train_label.with_columns(\n",
    "    pl.Series(name=\"prev_items\", values=prev_items_list)\n",
    ").rename(\n",
    "    {\"yad_no\":\"label\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "770b8510-8ac2-45a0-9e61-b9ee3cade385",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = generate_candidates(train, candidate_matrices_train)\n",
    "train = train.drop(\"prev_items\")\n",
    "train = add_label(train)\n",
    "train = filter_null(train, candidate_matrices_train)\n",
    "train = filter_session_not_include_positive(train)\n",
    "train = negative_sample(train)\n",
    "train = add_features(\n",
    "    train, \n",
    "    session_feat_train, product_feat_train, session_product_feat_train, \n",
    "    imf_model_train, user_id2index_train, item_id2index_train,\n",
    "    trend_imf_model_train, trend_user_id2index_train, trend_item_id2index_train\n",
    ")\n",
    "train = fill_null_and_cast(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "676f4d18-38c7-4d9a-8844-ac76633287a0",
   "metadata": {},
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b7e051-b835-4e28-ae84-e4ea839449a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "not_use_col = [\n",
    "    \"session_id\",\n",
    "    \"label\",\n",
    "    \"last_yad_no_1\",\n",
    "    \"last_yad_no_2\",\n",
    "    \"last_yad_no_3\",\n",
    "    \"candidate_yad_no\",\n",
    "    \"P_wid_cd\", \"P_ken_cd\", \"P_lrg_cd\", \"P_sml_cd\",\n",
    "    # \"S_wireless_lan_flg_last1\", \"S_wireless_lan_flg_last2\", \"S_wireless_lan_flg_last3\",\n",
    "    # \"S_onsen_flg_last1\", \"S_onsen_flg_last2\", \"S_onsen_flg_last3\",\n",
    "    # \"S_kd_stn_5min_last1\", \"S_kd_stn_5min_last2\", \"S_kd_stn_5min_last3\",\n",
    "    # \"S_kd_bch_5min_last1\", \"S_kd_bch_5min_last2\", \"S_kd_bch_5min_last3\",\n",
    "    # \"S_kd_slp_5min_last1\", \"S_kd_slp_5min_last2\", \"S_kd_slp_5min_last3\",\n",
    "    # \"S_kd_conv_walk_5min_last1\", \"S_kd_conv_walk_5min_last2\", \"S_kd_conv_walk_5min_last3\",\n",
    "    \"S_wid_cd_last1\", \"S_wid_cd_last2\", \"S_wid_cd_last3\",\n",
    "    \"S_ken_cd_last1\", \"S_ken_cd_last2\", \"S_ken_cd_last3\",\n",
    "    \"S_lrg_cd_last1\", \"S_lrg_cd_last2\", \"S_lrg_cd_last3\",\n",
    "    \"S_sml_cd_last1\", \"S_sml_cd_last2\", \"S_sml_cd_last3\",\n",
    "]\n",
    "FEATURES = [col for col in train.columns if col not in not_use_col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3546a5f0-72eb-4a27-9ab8-5821b4bd911e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train[[\"session_id\", \"candidate_yad_no\", \"label\"] + FEATURES]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02520907-405d-469b-bd89-e282d7ef996f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe7978cb-5e71-4342-b6a1-21c2a0794ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://qiita.com/birdwatcher/items/78e0158957fec2d6c9e8\n",
    "class DartEarlyStopping(object):\n",
    "    \"\"\"DartEarlyStopping\"\"\"\n",
    "\n",
    "    def __init__(self, data_name, monitor_metric, stopping_round):\n",
    "        self.data_name = data_name\n",
    "        self.monitor_metric = monitor_metric\n",
    "        self.stopping_round = stopping_round\n",
    "        self.best_score = None\n",
    "        self.best_model = None\n",
    "        self.best_score_list = []\n",
    "        self.best_iter = 0\n",
    "\n",
    "    def _is_higher_score(self, metric_score, is_higher_better):\n",
    "        if self.best_score is None:\n",
    "            return True\n",
    "        return (self.best_score < metric_score) if is_higher_better else (self.best_score > metric_score)\n",
    "\n",
    "    def _deepcopy(self, x):\n",
    "        # copy.deepcopyではlightgbmのモデルは完全にコピーされないためpickleを使用\n",
    "        return pickle.loads(pickle.dumps(x))\n",
    "\n",
    "    def __call__(self, env):\n",
    "        evals = env.evaluation_result_list\n",
    "        for data, metric, score, is_higher_better in evals:\n",
    "            if data != self.data_name or metric != self.monitor_metric:\n",
    "                continue\n",
    "            if not self._is_higher_score(score, is_higher_better):\n",
    "                if env.iteration - self.best_iter > self.stopping_round:\n",
    "                    # 終了させる\n",
    "                    eval_result_str = '\\t'.join([lgb.callback._format_eval_result(x) for x in self.best_score_list])\n",
    "                    lgb.basic._log_info(f\"Early stopping, best iteration is:\\n[{self.best_iter+1}]\\t{eval_result_str}\") \n",
    "                    lgb.basic._log_info(f\"You can get best model by \\\"DartEarlyStopping.best_model\\\"\")\n",
    "                    raise lgb.callback.EarlyStopException(self.best_iter, self.best_score_list)\n",
    "                return\n",
    "            # dartでは過去の木も更新されてしまうため、deepcopyしておく\n",
    "            self.best_model = self._deepcopy(env.model)\n",
    "            self.best_iter = env.iteration\n",
    "            self.best_score_list = evals\n",
    "            self.best_score = score\n",
    "            return\n",
    "        raise ValueError(\"monitoring metric not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea4b481-e835-4992-890d-abb7c59fb5f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "feat_imp = np.zeros((len(FEATURES),))\n",
    "oof_predictions = []\n",
    "gkf = GroupKFold(n_splits=K_FOLDS)\n",
    "for fold, (train_idx, val_idx) in enumerate(gkf.split(train, train[\"label\"], groups=train[\"session_id\"])):\n",
    "    print(f'======= Fold {fold+1}=======')\n",
    "\n",
    "    # Split data into train and validation sets\n",
    "    train_tmp = train.iloc[train_idx].copy()\n",
    "    valid_tmp = train.iloc[val_idx].copy()\n",
    "    train_tmp.sort_values(\"session_id\", inplace=True)\n",
    "    valid_tmp.sort_values(\"session_id\", inplace=True)\n",
    "    train_baskets = train_tmp.groupby(\"session_id\")[\"candidate_yad_no\"].count().values\n",
    "    valid_baskets = valid_tmp.groupby(\"session_id\")[\"candidate_yad_no\"].count().values\n",
    "    X_train, y_train = train_tmp[FEATURES], train_tmp[\"label\"]\n",
    "    X_val, y_val = valid_tmp[FEATURES], valid_tmp[\"label\"]\n",
    "\n",
    "    # Create LightGBM dataset\n",
    "    lgb_train = lgb.Dataset(X_train, y_train, group=train_baskets)\n",
    "    lgb_val = lgb.Dataset(X_val, y_val, group=valid_baskets, reference=lgb_train)\n",
    "\n",
    "    # Train LightGBM model\n",
    "    des = DartEarlyStopping(\"valid_1\", \"map@20\", 100)\n",
    "    model = lgb.train(LGBM_PARAMS,\n",
    "                    lgb_train,\n",
    "                    num_boost_round=1000,\n",
    "                    valid_sets=[lgb_train, lgb_val],\n",
    "                    callbacks=[\n",
    "                        lgb.early_stopping(stopping_rounds=100),\n",
    "                        lgb.log_evaluation(25),\n",
    "                        des,\n",
    "                    ]\n",
    "            )\n",
    "    model = des.best_model\n",
    "    \n",
    "    # Save LightGBM model\n",
    "    joblib.dump(model, f'../../model/lgb_{EXP_NAME}_{fold+1}.pkl')\n",
    "\n",
    "    # Add feature importance scores\n",
    "    feat_imp += model.feature_importance(importance_type=\"gain\")\n",
    "\n",
    "    # Predict oof\n",
    "    valid_sessions = list(valid_tmp[\"session_id\"].unique())\n",
    "    valid_log = original_train_log.filter(pl.col(\"session_id\").is_in(valid_sessions))\n",
    "    valid_label = original_train_label.filter(pl.col(\"session_id\").is_in(valid_sessions))\n",
    "    \n",
    "    prev_items_list = (\n",
    "        valid_log\n",
    "        .sort([\"session_id\", \"seq_no\"])\n",
    "        .group_by(\"session_id\", maintain_order=True)\n",
    "        .agg(pl.col(\"yad_no\"))\n",
    "    )[\"yad_no\"].to_list()\n",
    "    \n",
    "    valid = valid_label.with_columns(\n",
    "        pl.Series(name=\"prev_items\", values=prev_items_list)\n",
    "    ).rename(\n",
    "        {\"yad_no\":\"label\"}\n",
    "    )\n",
    "\n",
    "    valid = generate_candidates(valid, candidate_matrices_train)\n",
    "    valid = valid.drop(\"prev_items\")\n",
    "    valid = add_features(\n",
    "        valid, \n",
    "        session_feat_train, product_feat_train, session_product_feat_train, \n",
    "        imf_model_train, user_id2index_train, item_id2index_train,\n",
    "        trend_imf_model_train, trend_user_id2index_train, trend_item_id2index_train\n",
    "    )\n",
    "    valid = fill_null_and_cast(valid)\n",
    "\n",
    "    model = pickle.load(open(f'../../model/lgb_{EXP_NAME}_{fold+1}.pkl', \"rb\"))\n",
    "    preds = model.predict(valid[FEATURES].to_pandas(), num_iteration=model.best_iteration)\n",
    "    valid = valid.with_columns(pl.Series(name=\"pred\", values=preds))    \n",
    "    # valid = valid[[\"session_id\", \"candidate_yad_no\", \"pred\", \"label\"]]\n",
    "    valid = valid.sort([\"session_id\", \"pred\"], descending=[False, True])\n",
    "    oof_prediction = valid.group_by(\"session_id\", maintain_order=True).head(10)\n",
    "    oof_predictions.append(oof_prediction)\n",
    "\n",
    "oof_prediction = pl.concat(oof_predictions)\n",
    "oof_prediction = oof_prediction.with_columns((pl.col(\"candidate_yad_no\") == pl.col(\"label\")).cast(pl.Int8).alias(\"user_relevance\")).fill_null(0)\n",
    "user_relevances = oof_prediction.group_by(\"session_id\", maintain_order=True).all()[\"user_relevance\"].to_list()\n",
    "print(f'======= Training Done =======')\n",
    "total_num_session = original_train_log[\"session_id\"].n_unique()\n",
    "print(\"map@10:\", float(np.sum([ap_at_k(user_relevances, 10) for user_relevances in user_relevances]) / total_num_session))\n",
    "print(f'=============================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a525bcf-b01d-467c-bb14-9d474eef4d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize feature importances\n",
    "fi_df = pd.DataFrame(index=FEATURES)\n",
    "fi_df[\"importance\"] = feat_imp / K_FOLDS\n",
    "fi_df = fi_df.sort_values(by=\"importance\", ascending=False).head(25)\n",
    "sns.barplot(x=\"importance\", y=fi_df.index, data=fi_df).set(title=\"Feature Importance TOP25\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc2fa4d-9a85-40ae-aa4b-4cd86e226b57",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d72eed5f-bcda-4490-9663-c040e0e699d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "prev_items_list = (\n",
    "    test_log\n",
    "    .sort([\"session_id\", \"seq_no\"])\n",
    "    .group_by(\"session_id\", maintain_order=True)\n",
    "    .agg(pl.col(\"yad_no\"))\n",
    ")[\"yad_no\"].to_list()\n",
    "\n",
    "test = test_session.with_columns(\n",
    "    pl.Series(name=\"prev_items\", values=prev_items_list)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "655326a8-21dc-4a1d-a536-a3612ac15032",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = generate_candidates(test, candidate_matrices_test)\n",
    "test = test.drop(\"prev_items\")\n",
    "test = add_features(\n",
    "    test, \n",
    "    session_feat_test, product_feat_test, session_product_feat_test, \n",
    "    imf_model_test, user_id2index_test, item_id2index_test,\n",
    "    trend_imf_model_test, trend_user_id2index_test, trend_item_id2index_test\n",
    ")\n",
    "test = fill_null_and_cast(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6515b7e-8423-4174-b94b-e4f1eeddaf3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction\n",
    "\n",
    "def create_submit_file(pred, test_session, file_name, head_n=10):\n",
    "    pred = pred.sort([\"session_id\", \"pred\"], descending=[False, True])\n",
    "    prediction = pred.group_by(\"session_id\", maintain_order=True).head(head_n)\n",
    "    \n",
    "    col_map = {}\n",
    "    for i in range(0, head_n):\n",
    "        col_map[f\"{i}\"] = f\"predict_{i}\"   \n",
    "    prediction = prediction.with_columns(\n",
    "        prediction.select(pl.col(\"session_id\").cumcount().over(\"session_id\").alias(\"pred_num\"))\n",
    "    )\n",
    "    prediction = prediction.pivot(\n",
    "        index=\"session_id\",\n",
    "        columns = \"pred_num\",\n",
    "        values = \"candidate_yad_no\",\n",
    "    ).rename(col_map)\n",
    "    prediction = test_session.join(prediction, on=\"session_id\", how=\"left\")\n",
    "    prediction = prediction.fill_null(0)\n",
    "    prediction.drop(\"session_id\").write_csv(file_name)\n",
    "\n",
    "test = test.with_columns(\n",
    "    pl.Series(name=\"pred\", values=predict(test))\n",
    ")\n",
    "\n",
    "# for submit\n",
    "file_name = f\"../../output/{EXP_NAME}.csv\"\n",
    "create_submit_file(test, test_session, file_name, 10)\n",
    "\n",
    "# for ensamble\n",
    "file_name = f\"../../output/{EXP_NAME}_for_ensamble.csv\"\n",
    "create_submit_file(test, test_session, file_name, 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0079298b-5b72-4c05-acfa-01786ce6279d",
   "metadata": {},
   "source": [
    "# visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "346c2950-a9e1-4a37-8b57-6745b7fc608b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cols = FEATURES + [\"pred\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e66d0a5-9801-4bf8-b3ce-005f46d3d109",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for col in cols:\n",
    "#     fig, ax = plt.subplots()\n",
    "\n",
    "#     x1 = valid[col]\n",
    "#     x2 = test[col]\n",
    "\n",
    "#     # 最大値と最小値の間をn_bin等分した幅でヒストグラムの棒を表示するように設定（各targetのbin幅を統一する）\n",
    "#     n_bin = 20\n",
    "#     if col.startswith(\"co_visit_weight\"):\n",
    "#         x_max = 100\n",
    "#         x_min = 0\n",
    "#     elif col.startswith(\"popularity\"):\n",
    "#         x_max = 10\n",
    "#         x_min = 0\n",
    "#     elif col == \"seq_no_inverse\":\n",
    "#         x_max = 10\n",
    "#         x_min = 0\n",
    "#     elif col == \"imf_rank\":\n",
    "#         x_max = 10\n",
    "#         x_min = 0\n",
    "#     else:\n",
    "#         x_max = max(valid[col].max(), test[col].max())\n",
    "#         x_min = min(valid[col].min(), test[col].min())\n",
    "#     bins = np.linspace(x_min, x_max, n_bin)\n",
    "\n",
    "#     ax.hist(x1, density=True, bins=bins, color=\"red\", alpha=0.5, label=\"valid\")\n",
    "#     ax.hist(x2, density=True, bins=bins, color=\"blue\", alpha=0.5, label=\"test\")\n",
    "#     ax.set_title(col)\n",
    "#     ax.legend()\n",
    "\n",
    "#     fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d450d0-d0f8-48f1-b993-e136ad3d109a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66cad180-7ab8-4c15-b660-db2acbf0a029",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e0dab6-891c-4c86-bd05-71c30371d8fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a645299-8adb-487a-95b6-e809bd622498",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
